# ğŸ§  MatchGenie â€“ AI-Powered Resume Matcher (Ollama + Mistral)

This project is a smart, AI-powered resume screening tool that allows recruiters or hiring teams to compare resumes against job descriptions. It uses **local LLM inference via Ollama** running the `phi` model, providing fast and private resume analysis without sending data to the cloud.

## ğŸ” Features

- Upload multiple resumes (PDF format)
- Upload job descriptions (CSV format)
- Get AI-generated:
  - âœ… Match score
  - âœ… Candidate strengths
  - âœ… Weaknesses
  - âœ… Suggestions for improvement
- Semantic analysis using **Ollama + Mistral**
- Streamlit-based interactive UI
- Dashboard-style result display

---

## ğŸ“¸ Demo

![demo-screenshot](https://via.placeholder.com/800x400?text=Demo+Screenshot)

---

## âš™ï¸ Tech Stack

- ğŸ§  [Ollama](https://ollama.com) (Local LLM Runtime)
- ğŸ¤– `phi` model for semantic resume matching
- ğŸ“„ Streamlit (Frontend)
- ğŸ¼ Pandas (CSV handling)
- ğŸ“„ PyMuPDF (PDF parsing)

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/ai-resume-matcher.git
cd ai-resume-matcher
```

### 2ï¸âƒ£ Install Python Packages

Create a virtual environment (optional but recommended):

```bash
python -m venv venv
# Activate (Windows)
.env\Scriptsctivate
# Activate (Linux/Mac)
source venv/bin/activate
```

Then install dependencies:

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Install Ollama & Run Phi Model

#### ğŸ“¥ Install Ollama
Download and install Ollama:  
ğŸ‘‰ https://ollama.com/download

#### ğŸ§  Pull the `phi` Model

```bash
ollama pull mistral
```

#### â–¶ï¸ Start the model in background

```bash
ollama run mistral
```

This command runs the `mistral` model locally â€” it will stay running in the background to serve responses.

---

### 4ï¸âƒ£ Run the Streamlit App

```bash
streamlit run app.py
```

The app will open in your browser (usually at http://localhost:8501)

---

## ğŸ“‚ File Structure

```
ai-resume-matcher/
â”‚
â”œâ”€â”€ app.py                    # Main Streamlit app
â”œâ”€â”€ utils.py                  # Helper functions (score logic, PDF reader)
â”œâ”€â”€ ollama_client.py          # LLM query handler
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # You're reading it ğŸ™‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ job_description.csv   # Example job description
â”‚   â””â”€â”€ resumes/              # Folder to drop PDF resumes
â””â”€â”€ .gitignore
```

---

## ğŸ“Œ Notes

- This app is designed to work **offline** with **local AI inference** using Ollama.
- `mistral` is a smaller, efficient language model â€” great for local use without heavy GPU requirements.
- All resume/job data stays on your machine â€” no cloud API calls involved.

---

## ğŸ“œ License

MIT License Â© 2025 [Your Name]

---

## ğŸ™‹â€â™€ï¸ Contribution

PRs are welcome! Please open issues if you find bugs or want new features.

---

## â¤ï¸ Acknowledgements

- [Ollama](https://ollama.com) for providing easy-to-use LLMs locally
- [Streamlit](https://streamlit.io) for building elegant UIs with minimal code
